{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "eb08192d-ebf3-4fb1-9ec3-5e9cc7da3976",
    "_uuid": "90034f8a-e434-4c4a-933a-fd3ca157d5a0"
   },
   "source": [
    "---\n",
    "## <b>1 <span style='color:#3f4d63'>I</span> Introduction</b>\n",
    "---\n",
    "\n",
    "### **Competition's Goal**\n",
    "\n",
    "The goal of this competition is to assess the language proficiency of 8th-12th grade English Language Learners (ELLs). Utilizing a dataset of essays written by ELLs will help to develop proficiency models that better supports all students.\n",
    "\n",
    "Your work will help ELLs receive more accurate feedback on their language development and expedite the grading cycle for teachers. These outcomes could enable ELLs to receive more appropriate learning tasks that will help them improve their English language proficiency.\n",
    "\n",
    "![](https://www.caracteristicas.co/wp-content/uploads/2017/05/textos-literarios-1-e1569197016482.jpg)\n",
    "\n",
    "### **Data Description**\n",
    "\n",
    "The dataset presented here (the ELLIPSE corpus) comprises argumentative essays written by 8th-12th grade English Language Learners (ELLs). The essays have been scored according to six analytic measures: cohesion, syntax, vocabulary, phraseology, grammar, and conventions.\n",
    "\n",
    "Each measure represents a component of proficiency in essay writing, with greater scores corresponding to greater proficiency in that measure. The scores range from 1.0 to 5.0 in increments of 0.5. Your task is to predict the score of each of the six measures for the essays given in the test set.\n",
    "\n",
    "#### **Importing packages ⬇️**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6a58643d-cb45-4ec1-befd-2c9fdbb623f7",
    "_uuid": "d440a387-ca3d-4985-a836-91541788c2f2",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:23:48.85606Z",
     "iopub.status.busy": "2022-09-24T12:23:48.855706Z",
     "iopub.status.idle": "2022-09-24T12:23:48.863954Z",
     "shell.execute_reply": "2022-09-24T12:23:48.863222Z",
     "shell.execute_reply.started": "2022-09-24T12:23:48.856036Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display_html\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Basic libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy as sc\n",
    "from scipy import stats\n",
    "\n",
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cross Validation\n",
    "from sklearn.model_selection import KFold, cross_val_score, StratifiedKFold, learning_curve, train_test_split\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Plotly\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.offline as offline\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4b588101-fc60-4943-93d0-26c27cf78639",
    "_uuid": "27d179ea-ec44-4d0e-8c65-77bd4bec98d1"
   },
   "source": [
    "![](data:image/png;base64,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)\n",
    "\n",
    "### **Weights and Biases ⬇️**\n",
    "\n",
    "You will need a unique API key to log in to Weights & Biases.\n",
    "\n",
    "* If you don't have a Weights & Biases account, you can go to [https://wandb.ai/site](https://wandb.ai/site) and create a FREE account. Access your API key: [https://wandb.ai/authorize](https://wandb.ai/authorize).\n",
    "\n",
    "There are two ways you can login using a Kaggle kernel:\n",
    "\n",
    "* Run a cell with `wandb.login()`. It will ask for the API key, which you can `copy` + `paste` in.\n",
    "* You can also use Kaggle secrets to store your API key and use the code snippet below to login. Check out [this discussion post](https://www.kaggle.com/product-feedback/114053) to learn more about Kaggle secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8504bb93-7d0e-499a-8481-81172d8fc06e",
    "_uuid": "e51980ea-0e43-4725-991a-2ff3bf7bf369",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:23:48.889885Z",
     "iopub.status.busy": "2022-09-24T12:23:48.88869Z",
     "iopub.status.idle": "2022-09-24T12:23:58.839757Z",
     "shell.execute_reply": "2022-09-24T12:23:58.838659Z",
     "shell.execute_reply.started": "2022-09-24T12:23:48.889833Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "clear_output()\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    api_key = user_secrets.get_secret(\"wandb_api\")\n",
    "    wandb.login(key=api_key)\n",
    "    anony = None\n",
    "except:\n",
    "    anony = \"must\"\n",
    "    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ce9cafb6-b325-48d2-ae99-175c33d1ef75",
    "_uuid": "26a938d2-1355-4be9-8ce0-5d3cf35d592c"
   },
   "source": [
    "---\n",
    "## <b>2 <span style='color:#3f4d63'>I</span> Exploratory Data Analysis</b>\n",
    "---\n",
    "\n",
    "## **Basic Information**\n",
    "\n",
    "To start with we're going to get a first insight from the data we're given. In order to do so, we'll implement two different functions: \n",
    "\n",
    "* The first one is going to load each of the datasets given. \n",
    "* The second one will give us some basic information about each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4989a2e4-3cc2-4ef2-8c47-7f02b7145478",
    "_uuid": "0033c766-fe41-4183-a715-b1d3e2e1bfe9",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:23:58.841886Z",
     "iopub.status.busy": "2022-09-24T12:23:58.841585Z",
     "iopub.status.idle": "2022-09-24T12:23:58.972443Z",
     "shell.execute_reply": "2022-09-24T12:23:58.971215Z",
     "shell.execute_reply.started": "2022-09-24T12:23:58.841857Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    '''Load each of the datasets we are given.'''\n",
    "    \n",
    "    data_dir = Path(\"../input/feedback-prize-english-language-learning\")\n",
    "    train = pd.read_csv(data_dir / \"train.csv\")\n",
    "    test = pd.read_csv(data_dir / \"test.csv\")\n",
    "    sample_submission = pd.read_csv(data_dir / 'sample_submission.csv')\n",
    "    return train, test, sample_submission\n",
    "\n",
    "from termcolor import colored\n",
    "def data_info(csv, name=\"Train\"):\n",
    "    '''Prints basic information about the datasets we are given.'''\n",
    "    '''Inspired by: https://www.kaggle.com/code/andradaolteanu/rsna-fracture-detection-dicom-images-explore'''\n",
    "    \n",
    "    print(colored('==== {} ===='.format(name), 'blue', attrs=['bold']))\n",
    "    print(colored('Shape: ', 'blue', attrs=['bold']), csv.shape)\n",
    "    print(colored('NaN Values: ', 'blue', attrs=['bold']), csv.isnull().sum().sum(), '\\n')\n",
    "    #print(colored('Columns: ', 'blue', attrs=['bold']), list(csv.columns))\n",
    "    \n",
    "    display_html(csv.head())\n",
    "    if name != 'Sample Submission': print(\"\\n\")\n",
    "\n",
    "train, test, sample_submission = load_data()\n",
    "clear_output()\n",
    "\n",
    "names = [\"Train\", \"Test\", \"Sample Submission\"]\n",
    "for i, df in enumerate([train, test, sample_submission]): \n",
    "    data_info(df, names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "11e995a6-c7d9-4b86-87de-62c7915d4274",
    "_uuid": "824596e2-d67b-4722-a08d-23ad58b33f9d"
   },
   "source": [
    "* The essays are given a score for each of the seven analytic measures above: cohesion, etc. These analytic measures comprise the target for the competition. Therefore, let's analyse them a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f9817cc2-66bf-436c-91e3-cd1b1aaf321b",
    "_uuid": "a8687b8b-1c0d-43dc-938c-7f9fa9fd47a7",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:23:58.97359Z",
     "iopub.status.busy": "2022-09-24T12:23:58.973358Z",
     "iopub.status.idle": "2022-09-24T12:23:59.0166Z",
     "shell.execute_reply": "2022-09-24T12:23:59.015538Z",
     "shell.execute_reply.started": "2022-09-24T12:23:58.973567Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train.select_dtypes(['int','float']).describe().T.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8d433055-1691-4932-a217-42b147ed9098",
    "_uuid": "326b64c0-61d7-4bfa-9f65-62b07abb8ea1",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:23:59.019193Z",
     "iopub.status.busy": "2022-09-24T12:23:59.018903Z",
     "iopub.status.idle": "2022-09-24T12:24:00.129962Z",
     "shell.execute_reply": "2022-09-24T12:24:00.12888Z",
     "shell.execute_reply.started": "2022-09-24T12:23:59.019152Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "figure = plt.figure(figsize = (22,5))\n",
    "for i, col in enumerate(train.select_dtypes(['int','float']).columns): \n",
    "    ax = plt.subplot(1, 6, i+1)\n",
    "    if train[col].dtype == 'int': \n",
    "        sns.distplot(train[col], fit=stats.norm, color = 'red')        \n",
    "    else: \n",
    "        sns.distplot(train[col], fit=stats.norm)        \n",
    "    ax.set_ylim((0.0, 5.0))\n",
    "figure.tight_layout(h_pad=1.0, w_pad=0.5)\n",
    "plt.suptitle('Distribution Plots', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3814b197-262d-4e17-81ec-42925709328d",
    "_uuid": "8f9874dd-14bd-4e93-89d8-0cd7a0f11e58"
   },
   "source": [
    "* Essay example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dfc9481a-57f5-44fb-b247-341a361d471c",
    "_uuid": "a819c1fe-1fed-4626-a9c5-831b63470640",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:24:00.132744Z",
     "iopub.status.busy": "2022-09-24T12:24:00.131894Z",
     "iopub.status.idle": "2022-09-24T12:24:00.140963Z",
     "shell.execute_reply": "2022-09-24T12:24:00.139345Z",
     "shell.execute_reply.started": "2022-09-24T12:24:00.132682Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train['full_text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7c18c4ce-671d-437e-b577-ff5a606c04e6",
    "_uuid": "91f656a2-256c-421f-92b4-f97a76033644"
   },
   "source": [
    "* One thing that you may have noticed is that in the text there are some programming symbols like `\\n`. Let's start by getting rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9e9efc23-35fa-4828-8d1f-898fd6d0d304",
    "_uuid": "946a1e00-f651-4d46-b610-6ae8e7fbe9f6",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:24:00.143082Z",
     "iopub.status.busy": "2022-09-24T12:24:00.142762Z",
     "iopub.status.idle": "2022-09-24T12:24:00.245557Z",
     "shell.execute_reply": "2022-09-24T12:24:00.241866Z",
     "shell.execute_reply.started": "2022-09-24T12:24:00.143054Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "train['full_text'] = train[\"full_text\"].replace(re.compile(r'[\\n\\r\\t]'), ' ', regex=True)\n",
    "test['full_text'] = test[\"full_text\"].replace(re.compile(r'[\\n\\r\\t]'), ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a3869699-6639-4f0d-8491-5fa72481d230",
    "_uuid": "c592c0a1-d9c7-463e-8a4c-235ed872ebdd"
   },
   "source": [
    "## Character Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "94da348e-68e1-439d-9440-30be53b39013",
    "_uuid": "9a040a05-acfc-458e-b6a6-66eb1323f427",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:24:00.247604Z",
     "iopub.status.busy": "2022-09-24T12:24:00.247241Z",
     "iopub.status.idle": "2022-09-24T12:24:00.264879Z",
     "shell.execute_reply": "2022-09-24T12:24:00.260911Z",
     "shell.execute_reply.started": "2022-09-24T12:24:00.247575Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train['ncharacters'] = train['full_text'].str.len()\n",
    "avg_char = round(train['ncharacters'].mean())\n",
    "max_char = round(train['ncharacters'].max())\n",
    "print('Average length: {}'.format(avg_char))\n",
    "print('Max length: {}'.format(max_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "850cb60e-2aec-41db-b11b-b08f3a5a1867",
    "_uuid": "8b7948ee-0042-46e2-951b-7e9d5507fbb7",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:24:00.267705Z",
     "iopub.status.busy": "2022-09-24T12:24:00.266991Z",
     "iopub.status.idle": "2022-09-24T12:24:00.628895Z",
     "shell.execute_reply": "2022-09-24T12:24:00.628185Z",
     "shell.execute_reply.started": "2022-09-24T12:24:00.267666Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (22,5))\n",
    "sns.distplot(train['ncharacters'])\n",
    "plt.axvline(x = avg_char, color = 'red')\n",
    "plt.title('Character Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c40a6b93-8421-4ec9-809a-fca356542cab",
    "_uuid": "42c6e1fb-eb0d-428c-92d8-2ce4078d6597"
   },
   "source": [
    "## Word Count\n",
    "\n",
    "Let's look at the word count distribution across the dataset. The token count will inform settings for our model, like max sequence length and the types of model architectures we can use. Only some are suitable for very long sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b10ef25d-2f08-4475-b48d-50b54327cc5e",
    "_uuid": "2a92b8a7-3fe9-48b5-b18f-d34f35ce1b09",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:24:00.630831Z",
     "iopub.status.busy": "2022-09-24T12:24:00.630335Z",
     "iopub.status.idle": "2022-09-24T12:24:00.720572Z",
     "shell.execute_reply": "2022-09-24T12:24:00.719419Z",
     "shell.execute_reply.started": "2022-09-24T12:24:00.630802Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train['nwords'] = train['full_text'].apply(lambda x: len(x.split()))\n",
    "avg_words = round(train['nwords'].mean())\n",
    "max_words = round(train['nwords'].max())\n",
    "print('Average length: {}'.format(avg_words))\n",
    "print('Max length: {}'.format(max_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d1a50672-a8f2-4c68-89bb-4aaf64052584",
    "_uuid": "417b7033-dc93-49df-9e51-4bf5c2341fdc",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:24:00.725302Z",
     "iopub.status.busy": "2022-09-24T12:24:00.724865Z",
     "iopub.status.idle": "2022-09-24T12:24:01.050842Z",
     "shell.execute_reply": "2022-09-24T12:24:01.048711Z",
     "shell.execute_reply.started": "2022-09-24T12:24:00.725266Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (22,5))\n",
    "sns.distplot(train['nwords'])\n",
    "plt.axvline(x = avg_words, color = 'red')\n",
    "plt.title('Word count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e3d729d9-4417-4659-b90a-e76fc6bdedf9",
    "_kg_hide-input": true,
    "_uuid": "ccac38e8-d87b-435f-872c-9a9afafe0822",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:24:01.052587Z",
     "iopub.status.busy": "2022-09-24T12:24:01.052274Z",
     "iopub.status.idle": "2022-09-24T12:24:01.064013Z",
     "shell.execute_reply": "2022-09-24T12:24:01.061667Z",
     "shell.execute_reply.started": "2022-09-24T12:24:01.052561Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_distribution_per_score(c): \n",
    "    scores = ['cohesion','syntax','vocabulary','phraseology','grammar','conventions']\n",
    "    figure, axes = plt.subplots(nrows = 1, ncols = 6, figsize = (22,5))\n",
    "    for i, col in enumerate(scores):\n",
    "        conditionlist = [\n",
    "        (train[col] >= 4.5) ,\n",
    "        (train[col] >= 2) & (train[col] < 4.5),\n",
    "        (train[col] < 2)]\n",
    "        choicelist = ['High', 'Mid', 'Low']\n",
    "        train['performance'] = np.select(conditionlist, choicelist, default='Not Specified')\n",
    "\n",
    "        mask = train.performance != 'Mid'\n",
    "        sns.kdeplot(train[mask][c], hue = train.performance, ax = axes[i])\n",
    "        axes[i].set_title(col)\n",
    "\n",
    "        mask_low = train.performance == 'Low'\n",
    "        avg_low = train[mask_low][c].mean()\n",
    "        axes[i].axvline(x = avg_low, color = 'green', linestyle = '--')\n",
    "\n",
    "        mask_high = train.performance == 'High'\n",
    "        avg_high = train[mask_high][c].mean()\n",
    "        axes[i].axvline(x = avg_high, color = 'orange', linestyle = '--')\n",
    "\n",
    "        del train['performance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "91878fa3-95dc-494f-82af-1c196e3fc3f7",
    "_uuid": "c69c797d-35c7-4ecd-96c2-28a41f969dde"
   },
   "source": [
    "Let's observe how word count is distributed depending on scores. \n",
    "\n",
    "* Word count tend to be greater for those essays having great scores. Except for `grammar`, where both means are almost equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6a83d0bf-e51b-483b-bdd2-c2f58c3be134",
    "_uuid": "3cb16351-d517-47b4-97a5-ff890751dc47",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:24:01.066892Z",
     "iopub.status.busy": "2022-09-24T12:24:01.066461Z",
     "iopub.status.idle": "2022-09-24T12:24:02.186328Z",
     "shell.execute_reply": "2022-09-24T12:24:02.185064Z",
     "shell.execute_reply.started": "2022-09-24T12:24:01.066853Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_distribution_per_score('nwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "718a313c-ad3d-4ab7-8774-306e3abe2c85",
    "_uuid": "0b427e66-35ee-4fd4-b20e-e9e47c5de066"
   },
   "source": [
    "## **Sentence Count and Average Length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3b7e8d3d-68de-4f90-919a-136f339d6b75",
    "_uuid": "6ad20dab-7a90-43f5-80f1-7702a10a5b18",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:24:02.188205Z",
     "iopub.status.busy": "2022-09-24T12:24:02.187816Z",
     "iopub.status.idle": "2022-09-24T12:24:05.829884Z",
     "shell.execute_reply": "2022-09-24T12:24:05.828929Z",
     "shell.execute_reply.started": "2022-09-24T12:24:02.188148Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "train['sent_count'] = train['full_text'].apply(lambda x: len(sent_tokenize(x)))\n",
    "avg_sent = round(train['sent_count'].mean())\n",
    "print('Average Count: {}'.format(avg_sent))\n",
    "\n",
    "train['avg_sent_len'] = train['full_text'].apply(lambda x: np.mean([len(w.split()) for w in sent_tokenize(x)]))\n",
    "avg_sent_len = round(np.mean(train['avg_sent_len']))\n",
    "print('Average Sentence Length: {}'.format(avg_sent_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b50cc7c9-d0ee-4fc5-829c-32dd0fe1f8d5",
    "_uuid": "549afa51-3857-4311-b4f8-3005f649b169",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:24:05.831651Z",
     "iopub.status.busy": "2022-09-24T12:24:05.831259Z",
     "iopub.status.idle": "2022-09-24T12:24:06.386118Z",
     "shell.execute_reply": "2022-09-24T12:24:06.384085Z",
     "shell.execute_reply.started": "2022-09-24T12:24:05.831616Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (22,5))\n",
    "for i, col in enumerate(['sent_count','avg_sent_len']):\n",
    "        title = 'Sentence Count' if i == 0 else 'Average Sentence Length'\n",
    "        mean = avg_sent if i == 0 else avg_sent_len \n",
    "        ax = plt.subplot(1, 2, i+1)\n",
    "        sns.distplot(train[col])\n",
    "        ax.axvline(x = mean, color = 'red')\n",
    "        ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9074f06e-6c4b-4a52-b2d9-50207670f4a9",
    "_uuid": "81858267-350a-4d9a-b2a4-55e70437c346"
   },
   "source": [
    "Let's examine again scores. As we can appreciate below, low graded essays tend to have longer sentences in terms of words. This happens for each of the measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "30802b4b-79f5-49fe-a42f-23719dcf0c45",
    "_uuid": "08eb389b-f924-4261-8129-dd59bbafb92f",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:24:06.38818Z",
     "iopub.status.busy": "2022-09-24T12:24:06.387746Z",
     "iopub.status.idle": "2022-09-24T12:24:07.442853Z",
     "shell.execute_reply": "2022-09-24T12:24:07.441322Z",
     "shell.execute_reply.started": "2022-09-24T12:24:06.388131Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_distribution_per_score('avg_sent_len')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6a07b4b8-4b91-4783-a117-afc179040c2b",
    "_uuid": "9d0de6e4-88f0-4d5c-b9b5-eb19e01eb89a"
   },
   "source": [
    "## Stop Words\n",
    "\n",
    "> By **stop words** we are referring to a **set of commonly used words** in any language. For example, in English, “the”, “is” and “and”, would easily qualify as stop words. In NLP and text mining applications, stop words are used to get eliminated, allowing applications to focus on the important words instead.\n",
    ">\n",
    "> More info here -> [stop words tutorial](https://www.etutorialspoint.com/index.php/375-how-to-find-stop-words-in-nltk-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dd1424f5-2aa4-42ec-889a-5821f4960b27",
    "_uuid": "46950aeb-cf1b-4d4d-b2ff-d8505bfcc30f",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:24:07.445021Z",
     "iopub.status.busy": "2022-09-24T12:24:07.444586Z",
     "iopub.status.idle": "2022-09-24T12:24:07.453816Z",
     "shell.execute_reply": "2022-09-24T12:24:07.452381Z",
     "shell.execute_reply.started": "2022-09-24T12:24:07.444984Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "print(\"List of english stopwords -\")\n",
    "print(eng_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "97314c88-0189-4f18-b5b2-07231da78425",
    "_uuid": "c5ddd101-3fb4-4ed0-9e01-c4b2e85fff77",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:24:07.455938Z",
     "iopub.status.busy": "2022-09-24T12:24:07.455631Z",
     "iopub.status.idle": "2022-09-24T12:24:08.121065Z",
     "shell.execute_reply": "2022-09-24T12:24:08.119305Z",
     "shell.execute_reply.started": "2022-09-24T12:24:07.455913Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''Inspired by: https://www.kaggle.com/code/ryanluoli2/simple-text-analysis-to-start-with\n",
    "                https://www.etutorialspoint.com/index.php/375-how-to-find-stop-words-in-nltk-python'''\n",
    "\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "\n",
    "corpus = ''.join(train.full_text).split()\n",
    "dic = defaultdict(int)\n",
    "\n",
    "for w in corpus: \n",
    "    if w in eng_stopwords: \n",
    "        dic[w] += 1\n",
    "        \n",
    "dic_sorted = sorted(dic.items(), key = operator.itemgetter(1), reverse = True)\n",
    "x, y = zip(*dic_sorted[:10])\n",
    "plt.bar(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1c340798-b86a-4f32-a58b-df1f09ed2e34",
    "_uuid": "9a375921-39d5-4edd-8d90-ed3dbbfbc85d"
   },
   "source": [
    "## Most Frequent N-Grams\n",
    "\n",
    "> **What’s an n-gram?** It’s a sequence of n words in a text. For example, the bi-grams (n = 2) for the sentence “How are you today?” would be: “How are”, “are you”, and “you today”. The tri-grams (n =3) would be “How are you” and “are you today”.\n",
    ">\n",
    "> For more detailed explanation -> [Fundamental EDA Techniques for NLP](https://towardsdatascience.com/fundamental-eda-techniques-for-nlp-f81a93696a75)\n",
    "\n",
    "\n",
    "Before we begin, we need to preprocess the text by changing everything to lowercase, and removing all punctuations and non-Roman characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2b79c1d1-e29b-40f3-a274-4946f23592d5",
    "_uuid": "174853d7-5d6d-4ee7-88c6-44599398d773",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:24:08.12402Z",
     "iopub.status.busy": "2022-09-24T12:24:08.123556Z",
     "iopub.status.idle": "2022-09-24T12:24:08.661903Z",
     "shell.execute_reply": "2022-09-24T12:24:08.660722Z",
     "shell.execute_reply.started": "2022-09-24T12:24:08.123984Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(\"[%s]\" % re.escape(string.punctuation), \"\", text)\n",
    "    # Remove non-Roman characters\n",
    "    text = re.sub(\"([^\\x00-\\x7F])+\", \" \", text)    \n",
    "    return text\n",
    "\n",
    "train_copy = train.copy()\n",
    "train_copy['full_text'] = train_copy['full_text'].map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7b3bcf72-acbf-4bc7-8444-ade2f0ce09c0",
    "_uuid": "dba09067-f41f-4dba-b771-b0eb68e035bf"
   },
   "source": [
    "The cleaned review texts look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fe44ef40-8f2d-44f9-92b9-6d1641e0fd57",
    "_uuid": "a9889fb5-73d3-4a65-ba9b-4e27fd90f88f",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:24:08.664336Z",
     "iopub.status.busy": "2022-09-24T12:24:08.663733Z",
     "iopub.status.idle": "2022-09-24T12:24:08.671338Z",
     "shell.execute_reply": "2022-09-24T12:24:08.669843Z",
     "shell.execute_reply.started": "2022-09-24T12:24:08.664308Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_copy['full_text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d85221cd-658c-40af-ae25-e22f9725ebae",
    "_uuid": "adb27a28-cce1-4a58-ba9e-8404e3c50171"
   },
   "source": [
    "To get the most frequent words, we first need to create a so-called “corpus”. That means we create a list containing all relevant words from the cleaned review texts. By “relevant” word, I mean words that aren’t stopwords. Let's write a function to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e36344e8-5dd1-4d5d-81f0-7c519d17fa6c",
    "_kg_hide-input": true,
    "_uuid": "58662d51-736e-4051-8c04-404a7e4a3bb6",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:24:08.673566Z",
     "iopub.status.busy": "2022-09-24T12:24:08.672663Z",
     "iopub.status.idle": "2022-09-24T12:24:08.68159Z",
     "shell.execute_reply": "2022-09-24T12:24:08.680295Z",
     "shell.execute_reply.started": "2022-09-24T12:24:08.673537Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def most_freq_ngrams(s1 = 2, s2 = 2):\n",
    "    '''Return most frequent N-Grams for a size given.'''\n",
    "    \n",
    "    # Initialize CountVectorizer\n",
    "    vec = CountVectorizer(stop_words = eng_stopwords, ngram_range = (s1, s2))\n",
    "\n",
    "    # Matrix of ngrams\n",
    "    bow = vec.fit_transform(train_copy[\"full_text\"])\n",
    "\n",
    "    # Count frequency of ngrams\n",
    "    count_values = bow.toarray().sum(axis=0)\n",
    "\n",
    "    # Create DataFrame from ngram frequencies\n",
    "    ngram_freq = pd.DataFrame(sorted([(count_values[i], k) for k, i in vec.vocabulary_.items()], reverse = True))\n",
    "    ngram_freq.columns = [\"frequency\", \"ngram\"]\n",
    "    \n",
    "    return ngram_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d435169c-933c-4529-b1f3-1aaffa215201",
    "_uuid": "bb5b0c70-4a0a-4f81-8188-4a5ec86bee89",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:24:08.68345Z",
     "iopub.status.busy": "2022-09-24T12:24:08.683042Z",
     "iopub.status.idle": "2022-09-24T12:24:31.812141Z",
     "shell.execute_reply": "2022-09-24T12:24:31.811205Z",
     "shell.execute_reply.started": "2022-09-24T12:24:08.683423Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (22, 6))\n",
    "for i in range(2,4):\n",
    "    ngram_freq = most_freq_ngrams(i, i)\n",
    "    sns.barplot(data = ngram_freq[:10], x = 'frequency', y = 'ngram', ax = axes[i-2])\n",
    "    del ngram_freq\n",
    "    \n",
    "figure.tight_layout(h_pad=1.0, w_pad=0.5)\n",
    "plt.suptitle('Most Frequent N-Grams', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8d460b6d-ecca-403d-bb5b-83d2cadca291",
    "_uuid": "3f1c303a-e1ec-4618-b0b4-58e5457d12c9"
   },
   "source": [
    "## **Part of Speech Tagging (POS)**\n",
    "\n",
    "We’ll look at Part-of-Speech (POS) tagging and how to use it to get the most frequent adjectives, nouns, verbs, etc. In other words, with POS tagging, we are able to refine the EDA on the most frequent terms. E.g., you could explore, which adjectives or verbs are most common. POS tagging takes every token in a text and categorizes it as nouns, verbs, adjectives, and so on, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c0f77aa1-576a-447a-a17d-da2860e3a163",
    "_kg_hide-input": true,
    "_uuid": "b62ea1fc-0f3a-4c90-ad22-c51a36a21454",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:24:31.81428Z",
     "iopub.status.busy": "2022-09-24T12:24:31.813918Z",
     "iopub.status.idle": "2022-09-24T12:24:31.824723Z",
     "shell.execute_reply": "2022-09-24T12:24:31.823901Z",
     "shell.execute_reply.started": "2022-09-24T12:24:31.814242Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_tokens(tags, tag_type = 'ADJ'): \n",
    "    '''Returns all the tokens that are tagged as \"tag\" in tags. '''\n",
    "    \n",
    "    t = [word for word, pos in tags if ((pos == tag_type) & ( word not in eng_stopwords))]\n",
    "    return t\n",
    "\n",
    "def show_most_common_tokens(tags, tag_type = 'ADJ', row = -1, col = -1): \n",
    "    '''Calculate the most commont tokens tagged as \"tag_type\" in tags. \n",
    "       Shows a Bar Plot with the results. '''\n",
    "    \n",
    "    adj_tags = get_tokens(tags, tag_type)\n",
    "    \n",
    "    # Count most common adjectives\n",
    "    most_common = Counter(adj_tags).most_common(10)\n",
    "\n",
    "    # Visualize most common tags as bar plots\n",
    "    words, frequency = [], []\n",
    "    for word, count in most_common:\n",
    "        words.append(word)\n",
    "        frequency.append(count)\n",
    "\n",
    "    if col == -1: sns.barplot(x = frequency, y = words)\n",
    "    else: sns.barplot(x = frequency, y = words, ax = axes[row][col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f15c0267-182e-4003-9b0e-c0d1dd76f99f",
    "_uuid": "eafe85ab-bc24-43af-8a3c-0095aa9bf5b4"
   },
   "source": [
    "* Quick example: below we are showing most common adjectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ac163f21-d35e-450a-9512-1cfcba1f6258",
    "_uuid": "6abe5201-7ca9-4baf-9a9f-cdae380d31e7",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:24:31.826421Z",
     "iopub.status.busy": "2022-09-24T12:24:31.826093Z",
     "iopub.status.idle": "2022-09-24T12:25:18.363334Z",
     "shell.execute_reply": "2022-09-24T12:25:18.361439Z",
     "shell.execute_reply.started": "2022-09-24T12:24:31.826393Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "corpus_without_stop = [w for w in corpus if w not in eng_stopwords]\n",
    "tags = nltk.pos_tag(corpus_without_stop, tagset = \"universal\")\n",
    "\n",
    "show_most_common_tokens(tags, 'ADJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2691f026-aea6-43d7-bdf6-a316411efe49",
    "_uuid": "bb266839-f801-4914-bd54-d75d433dc89a"
   },
   "source": [
    "Hereafter, we are going to compare the most common tokens depending on each of the scores. The aim of this is to analyse which type of words appear more in low and high graded essays. \n",
    "\n",
    "Let's begin with **verbs**.\n",
    "\n",
    "* We appreciate that auxiliary verb **would** leads every plot when talking about **high grades**. \n",
    "* The same happens in low graded plot, but with verb **go**. \n",
    "* Main difference between low and high graded essays' verbs: \n",
    "    * In high graded ones, there are **auxiliary verbs** like would, could, may, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "54012329-35d3-4a56-b909-e3887d8b083e",
    "_kg_hide-input": true,
    "_uuid": "5f680a06-decb-400c-8f02-a41b25d012a1",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:25:18.365874Z",
     "iopub.status.busy": "2022-09-24T12:25:18.365557Z",
     "iopub.status.idle": "2022-09-24T12:25:35.819263Z",
     "shell.execute_reply": "2022-09-24T12:25:35.817477Z",
     "shell.execute_reply.started": "2022-09-24T12:25:18.365848Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "scores = ['cohesion','syntax','vocabulary','phraseology','grammar','conventions']\n",
    "figure, axes = plt.subplots(nrows = 6, ncols = 2, figsize = (22,25))\n",
    "\n",
    "for i, col in enumerate(scores):\n",
    "    for j in range(2):\n",
    "        \n",
    "        t = 'Low' if j == 0 else 'High'\n",
    "        title = '{} Graded - {}'.format(t, col)\n",
    "        mask = train[col] < 2 if j == 0 else train[col] > 4\n",
    "        \n",
    "        corpus = ''.join(train[mask].full_text).split()\n",
    "        corpus_without_stop = [w for w in corpus if w not in eng_stopwords]\n",
    "        tags = nltk.pos_tag(corpus_without_stop, tagset = \"universal\")\n",
    "        show_most_common_tokens(tags, 'VERB', i, j)\n",
    "        axes[i,j].set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b9e80efd-0abb-4c4b-b387-6eb58f7a367a",
    "_uuid": "5c326aa7-5852-4ad8-a80b-562dc0937d6d"
   },
   "source": [
    "Now, it's **nouns** turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c7a3bf7e-a102-4599-96fa-421d6f3b2c26",
    "_kg_hide-input": true,
    "_uuid": "58de3741-e2b5-44f8-8958-eb6d5754bc60",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:25:35.821641Z",
     "iopub.status.busy": "2022-09-24T12:25:35.821363Z",
     "iopub.status.idle": "2022-09-24T12:25:52.836451Z",
     "shell.execute_reply": "2022-09-24T12:25:52.835203Z",
     "shell.execute_reply.started": "2022-09-24T12:25:35.821617Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(nrows = 6, ncols = 2, figsize = (22,25))\n",
    "for i, col in enumerate(scores):\n",
    "    for j in range(2):\n",
    "        \n",
    "        t = 'Low' if j == 0 else 'High'\n",
    "        title = '{} Graded - {}'.format(t, col)\n",
    "        mask = train[col] < 2 if j == 0 else train[col] > 4\n",
    "        \n",
    "        corpus = ''.join(train[mask].full_text).split()\n",
    "        corpus_without_stop = [w for w in corpus if w not in eng_stopwords]\n",
    "        tags = nltk.pos_tag(corpus_without_stop, tagset = \"universal\")\n",
    "        show_most_common_tokens(tags, 'NOUN', i, j)\n",
    "        axes[i,j].set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d57fba6c-cf5d-4eca-b684-c7475dc8c59e",
    "_uuid": "4bdfb46e-60e5-4a32-9b11-8076a717e407"
   },
   "source": [
    "## **Correlations**\n",
    "\n",
    "By examining the following chart we can conclude the following: \n",
    "\n",
    "* High correlation between scoring measurements suggests that essays tend to have similar grades in each of the measurements. \n",
    "\n",
    "* `Avg_sent_len` is negatively correlated with every scoring measure. Thus, we can conclude that very long phrases could have a negative impact on how an essay is graded. This make sense when we observe `sent_count` as well. The shorter the phrases are, the more amount of phrases an essay has. That's the reason for its positive correlation with the scoring measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4870d20d-9b1b-4141-8dfd-0af2097e6a51",
    "_uuid": "e9edc0b6-bba8-43b3-a9fd-1c67ba1d5f59",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:25:52.83821Z",
     "iopub.status.busy": "2022-09-24T12:25:52.837872Z",
     "iopub.status.idle": "2022-09-24T12:25:53.914722Z",
     "shell.execute_reply": "2022-09-24T12:25:53.913211Z",
     "shell.execute_reply.started": "2022-09-24T12:25:52.838153Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "corr= train.select_dtypes(['int','float']).corr()\n",
    "# Getting the Upper Triangle of the co-relation matrix\n",
    "matrix = np.triu(corr)\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (22,8))\n",
    "# Heatmap without absolute values\n",
    "sns.heatmap(corr, mask=matrix, center = 0, cmap = 'vlag', ax = axes[0], \n",
    "            annot=True, fmt='.2f').set_title('Without absolute values')\n",
    "# Heatmap with absolute values\n",
    "sns.heatmap(abs(corr), mask=matrix, center = 0, cmap = 'vlag', ax = axes[1], \n",
    "           annot=True, fmt='.2f').set_title('With absolute values')\n",
    "\n",
    "fig.tight_layout(h_pad=1.0, w_pad=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bd419e9d-a7a8-46b0-843f-dedd96b41349",
    "_uuid": "1093a984-8d26-4173-81d8-642dddf20a80"
   },
   "source": [
    "## **Lemmatization**\n",
    "\n",
    "In simpler terms, it is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*Kt9AbfaCIHCG2QjBckRVLg.png)\n",
    "\n",
    "* Quick Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e036e2f4-b5b9-4271-ac91-8d03de0025f1",
    "_uuid": "1b03d8d5-4ff6-4a10-9c77-65d999d296ac",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:25:53.916884Z",
     "iopub.status.busy": "2022-09-24T12:25:53.916476Z",
     "iopub.status.idle": "2022-09-24T12:25:56.432733Z",
     "shell.execute_reply": "2022-09-24T12:25:56.431572Z",
     "shell.execute_reply.started": "2022-09-24T12:25:53.916848Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Importing Lemmatizer library from nltk\n",
    "nltk.download('omw-1.4')\n",
    "clear_output()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    " \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "20dc8170-7b81-4c91-a590-b3a990e0fad0",
    "_uuid": "2632cc58-1b3a-459b-b0a0-aae4c4be1a45"
   },
   "source": [
    "Let's **compare** now which **lemmas** are the most common ones in essays with a perfect grade (5.0) in **vocabulary and syntax**, and the ones with a low grade (less or equal than 2.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "92c55b3a-5d40-40a0-9cb7-d12017252951",
    "_uuid": "109f4d64-bfad-452a-9348-0bdb137a09c1",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:25:56.437303Z",
     "iopub.status.busy": "2022-09-24T12:25:56.436957Z",
     "iopub.status.idle": "2022-09-24T12:25:56.444667Z",
     "shell.execute_reply": "2022-09-24T12:25:56.443223Z",
     "shell.execute_reply.started": "2022-09-24T12:25:56.437278Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def most_common_lemmas(corpus): \n",
    "    '''Returns a dictionary with lemmas, sorted by frequency.'''\n",
    "    \n",
    "    dic = defaultdict(int)\n",
    "    for w in corpus:\n",
    "        lemma = lemmatizer.lemmatize(w)\n",
    "        dic[lemma] += 1\n",
    "\n",
    "    dic_sorted = sorted(dic.items(), key = operator.itemgetter(1), reverse = True)\n",
    "    return dic_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "65b71a82-eead-4953-b290-6e0598f55419",
    "_uuid": "4d8da8b2-469e-47de-83ef-05e78baf585a"
   },
   "source": [
    "**Insights**\n",
    "\n",
    "* Most common lemmas are shared between both types of essays. \n",
    "* However, from top 4/5 onwards the difference in technical lemmas start to be significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "efbe2b3e-3af7-4944-b761-e6b65f1d7a34",
    "_uuid": "e1f94d22-cf60-4e17-b054-f933ab0569c8",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:25:56.446399Z",
     "iopub.status.busy": "2022-09-24T12:25:56.445922Z",
     "iopub.status.idle": "2022-09-24T12:25:57.932136Z",
     "shell.execute_reply": "2022-09-24T12:25:57.931105Z",
     "shell.execute_reply.started": "2022-09-24T12:25:56.446362Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(nrows = 2, ncols = 2, figsize = (22,8))\n",
    "for i, col in enumerate(['syntax','vocabulary']):    \n",
    "    for j in range(2):\n",
    "        t = 'Low' if j == 0 else 'High'\n",
    "        title = '{} Graded - {}'.format(t, col)\n",
    "        mask = train_copy[col] <= 2 if j == 0 else train_copy[col] == 5\n",
    "\n",
    "        corpus = ''.join(train_copy[mask].full_text).split()\n",
    "        corpus_without_stop = [w for w in corpus if w not in eng_stopwords]\n",
    "        dic_sorted = most_common_lemmas(corpus_without_stop)\n",
    "        x, y = zip(*dic_sorted[:15])\n",
    "        axes[i, j].bar(x,y)\n",
    "        axes[i, j].set_title(title)\n",
    "\n",
    "figure.tight_layout(h_pad=1.0, w_pad=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2d763185-1a6d-4e7a-90c3-5bc7aafb7a74",
    "_uuid": "1a5e5d2f-3ac7-4d48-8b31-5ff8246d297f"
   },
   "source": [
    "## **Exploring through Text Complexity. Flesch Reading Ease (FRE)**\n",
    "\n",
    "> Tutorial > [EDA for NLP](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools)\n",
    "\n",
    "Higher scores indicate material that is easier to read, lower numbers mark harder-to-read passages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8d172ea4-56f2-4703-b740-6ab185a410ad",
    "_uuid": "fef7077c-7c56-46b9-a067-1b0b09b0acfd",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:25:57.933803Z",
     "iopub.status.busy": "2022-09-24T12:25:57.933376Z",
     "iopub.status.idle": "2022-09-24T12:26:12.042227Z",
     "shell.execute_reply": "2022-09-24T12:26:12.041146Z",
     "shell.execute_reply.started": "2022-09-24T12:25:57.933778Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install textstat\n",
    "clear_output()\n",
    "from textstat import flesch_reading_ease\n",
    "\n",
    "train['complexity'] = train['full_text'].apply(lambda x : flesch_reading_ease(x))\n",
    "plt.figure(figsize = (8,5))\n",
    "sns.distplot(train['complexity'])\n",
    "plt.title('Text Complexity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c6e0ea8f-1c1f-4e4a-b7f3-2fb975261559",
    "_uuid": "26d42396-5660-43af-a413-4bf33d5f2825"
   },
   "source": [
    "Let's make a brief comparison depending on the complexity scores given to a text. We're gonna start by examining one with a relatively low score (between 10 - 20). That means, hard to read for a kid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6b929f8c-4a70-47b9-b9d3-5af7c9c1c00c",
    "_uuid": "23f00217-5997-488c-a2d1-51181fc54bcc",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:26:12.044376Z",
     "iopub.status.busy": "2022-09-24T12:26:12.0437Z",
     "iopub.status.idle": "2022-09-24T12:26:12.053352Z",
     "shell.execute_reply": "2022-09-24T12:26:12.052219Z",
     "shell.execute_reply.started": "2022-09-24T12:26:12.044334Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train['full_text'][3039]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a55f2cc9-9a98-4d25-aea0-0d008cb25aed",
    "_uuid": "4ceec666-a255-485c-886f-026487fb28be"
   },
   "source": [
    "By contrast, let's examine some essays that were given a pretty high score, in terms of complexity. To recap, those are the easiest essays to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1f0d51ca-02ca-4fca-b669-e0899157195f",
    "_uuid": "80989417-090d-4fb9-b029-6d679d012837",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:26:12.055524Z",
     "iopub.status.busy": "2022-09-24T12:26:12.055136Z",
     "iopub.status.idle": "2022-09-24T12:26:12.071338Z",
     "shell.execute_reply": "2022-09-24T12:26:12.070268Z",
     "shell.execute_reply.started": "2022-09-24T12:26:12.055488Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train['full_text'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "94f4a638-9da1-4cea-baef-ce2ce81cc483",
    "_uuid": "dbf9e592-408e-41b6-9c64-611912b06610"
   },
   "source": [
    "* The difference in complexity is clear between these two texts. \n",
    "\n",
    "However, while most essays have obtained a complexity score between 0 - 100, one incredible insight about the chart we showed before is the ridiculously low score that some texts are given. Let's examine a few of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9854ce98-d5d3-4eaf-8ae7-a51723903a0b",
    "_uuid": "a57a16f7-b1e1-44ed-a103-0782cf250b41",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:26:12.073294Z",
     "iopub.status.busy": "2022-09-24T12:26:12.072572Z",
     "iopub.status.idle": "2022-09-24T12:26:12.084072Z",
     "shell.execute_reply": "2022-09-24T12:26:12.082623Z",
     "shell.execute_reply.started": "2022-09-24T12:26:12.073266Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train['full_text'][39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "40bba8f1-50c4-4719-8eeb-35d07a421fb3",
    "_uuid": "e502268f-0020-488d-9104-df0f83555cce",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:26:12.086097Z",
     "iopub.status.busy": "2022-09-24T12:26:12.085649Z",
     "iopub.status.idle": "2022-09-24T12:26:12.097557Z",
     "shell.execute_reply": "2022-09-24T12:26:12.096229Z",
     "shell.execute_reply.started": "2022-09-24T12:26:12.08606Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train['full_text'][272]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5b7ac2f0-47b6-4ad8-bf05-e738c7ad0dc4",
    "_uuid": "34eab8df-b5c7-4b21-87de-4a78e1173d96"
   },
   "source": [
    "* As we have appreciated those kind of texts are almost **unreadable**: \n",
    "    * In the first one, lots of **bad-written words**.\n",
    "    * The second one uses some non-sense and **incorrect grammatical structures**. \n",
    "    * Both essays have something in common. **There are no punctuation marks !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "643b9e91-3c66-40d4-8c24-2693e226406e",
    "_uuid": "40edceeb-9634-4183-ba12-dac397df7ee1"
   },
   "source": [
    "## **Sentiment Analysis**\n",
    "\n",
    "To perform sentiment analysis we'll make use of `TextBlob`. For more detailed information, take a look onto this -> [Tutorial: TextBlob QuickStart](https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f94a9eed-8b5b-43e5-bfbe-3e62233c37ab",
    "_uuid": "546aa452-c110-4e95-a97c-c85d7ea01cb3",
    "execution": {
     "iopub.execute_input": "2022-09-24T12:26:12.100654Z",
     "iopub.status.busy": "2022-09-24T12:26:12.10039Z",
     "iopub.status.idle": "2022-09-24T12:26:30.556304Z",
     "shell.execute_reply": "2022-09-24T12:26:30.554721Z",
     "shell.execute_reply.started": "2022-09-24T12:26:12.10063Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "train['polarity'] = train['full_text'].apply(lambda x: TextBlob(x).sentiment[0])\n",
    "train['subjetivity'] = train['full_text'].apply(lambda x: TextBlob(x).sentiment[1])\n",
    "figure, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (16,4))\n",
    "sns.kdeplot(train['polarity'], ax = axes[0])\n",
    "axes[0].set_title('Polarity Distribution')\n",
    "sns.kdeplot(train['subjetivity'], ax = axes[1])\n",
    "axes[1].set_title('Subjetivity Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6030fc45-4a9c-4ee8-b6ee-917a0860e11d",
    "_uuid": "6bb7884a-a5da-42b5-9731-af18de773f98"
   },
   "source": [
    "---\n",
    "## <b>3 <span style='color:#3f4d63'>I</span> Modeling</b>\n",
    "---\n",
    "\n",
    "Due to competition's submission requirements, I'll create a **new notebook for the whole modeling and submission section. You can continue watching the project here** -> [English language learning | Deberta with W&B](https://www.kaggle.com/javigallego/english-language-learning-deberta-with-w-b). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
