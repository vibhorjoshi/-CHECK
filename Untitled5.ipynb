{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyMUg7ZPmX/C/3fKDQLTuoPe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vibhorjoshi/-CHECK/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *data prepration* and *loading*"
      ],
      "metadata": {
        "id": "9zgdboz2nnLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers requests numpy torch sentence-transformers"
      ],
      "metadata": {
        "id": "KiiUVcrSTlZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "eF_o9rNGUJJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes accelerate transformers torch"
      ],
      "metadata": {
        "id": "fLFIqJeMeGUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "8XBGpUN7gAJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, pipeline\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import Dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import optuna\n",
        "from sklearn.model_selection import train_test_split\n",
        "from io import StringIO"
      ],
      "metadata": {
        "id": "KRxRciJwfe5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "X2jX79GuixKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name = \"unsloth/llama-3-8b-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load without quantization (requires 16GB+ VRAM)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"âœ… Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "6r8qUm1Af-T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Test the model\n",
        "def generate_text(prompt, max_length=100):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Test generation\n",
        "test_prompt = \"The future of AI is\"\n",
        "result = generate_text(test_prompt)\n",
        "print(f\"\\nðŸš€ Test Generation:\")\n",
        "print(f\"Prompt: {test_prompt}\")\n",
        "print(f\"Output: {result}\")\n",
        "\n",
        "# For chat-style interactions (if using Instruct model)\n",
        "# Note: If you switched from an Instruct model, this function might not behave as expected\n",
        "def chat_with_llama(message):\n",
        "    chat_prompt = f\"<|start_header_id|>user<|end_header_id|>\\n{message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
        "    response = generate_text(chat_prompt, max_length=200)\n",
        "    # Extract just the assistant's response\n",
        "    assistant_response = response.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\")[-1]\n",
        "    return assistant_response.split(\"<|eot_id|>\")[0].strip()\n",
        "\n",
        "# Example usage for instruct models\n",
        "# Check if the loaded model name indicates it's an Instruct model before attempting chat\n",
        "if \"Instruct\" in model_name.lower():\n",
        "    chat_response = chat_with_llama(\"Explain quantum computing in simple terms.\")\n",
        "    print(f\"\\nðŸ’¬ Chat Example:\")\n",
        "    print(f\"User: Explain quantum computing in simple terms.\")\n",
        "    print(f\"Assistant: {chat_response}\")\n",
        "else:\n",
        "    print(\"\\nðŸ’¬ Skipping Chat Example: Loaded model is not an Instruct model.\")"
      ],
      "metadata": {
        "id": "dPgMvk0VeBDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *fetching* *live  api key data*"
      ],
      "metadata": {
        "id": "T535sq6T5JzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# API Keys (replace)\n",
        "OCM_API_KEY = \"8ea88008-6ca4-40f1-89af-d66d1cc66cb5\"\n",
        "GOOGLE_API_KEY = \"AIzaSyCRDsy_Bt5UZO2Slho8bxLAmB6du5sdmiU\"\n",
        "\n",
        "# Weather API: Use Open-Meteo (free, no key, from tool snippets)\n",
        "def fetch_weather(lat, lon):\n",
        "    url = f\"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current=weather_code,temperature\"\n",
        "    try:\n",
        "        resp = requests.get(url).json()\n",
        "        code = resp['current']['weather_code']  # WMO code: 0=sunny, 3=overcast, 51=rain, etc.\n",
        "        temp = resp['current']['temperature']\n",
        "        # Map to score: Good weather (sunny/clear) = 1.0, bad (rain/snow) = 0.5\n",
        "        weather_score = 1.0 if code < 3 else 0.5  # Simple; can expand\n",
        "        return weather_score, temp\n",
        "    except:\n",
        "        return 0.8, 20.0  # Mock fallback"
      ],
      "metadata": {
        "id": "gnS8v7U5p_10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining live data\n",
        "\n",
        "#### 1)Combined Raw Dataset\n",
        "Here's the unified raw dataset (CSV format) with the 5 features.\n",
        "\n",
        "I processed via code simulation (based on samples; full would require\n",
        "downloading, but this mimics).\n",
        "Rows from each dataset, mapped:\n",
        "Station Name: From charger_id/station_id/ChargerID.\n",
        "Distance (km): From poi_proximity_km/spatial_proximity_km/additional_info_driving_pattern_km (or NaN).\n",
        "Power Level (kW): From ChargerCapacity/volume_kwh/energy_kwh (indirect; normalized to kW where possible).\n",
        "Rating: From preference_rating (or NaN).\n",
        "Cost: From electricity_price_usd/price_usd (or NaN).\n",
        "\n",
        "#### 2) Since no dataset has all 5 features exactly, I'll proceed to combine similar\n",
        "\n",
        "\n",
        "datasets (1-7) into a raw, unified dataset with the 5 columns. I used the provided sample data (CSV headers + example rows) from your query, as full CSVs are large and not directly raw-downloadable without registration (e.g., figshare/ACN require tokens; GitHub repos have zipped CSVs). From tool searches:\n",
        "\n",
        "1)\n",
        "CHARGED: Repo at https://github.com/IntelligentSystemsLab/CHARGED (CSVs in /data folder, but no direct raw; e.g., shenzhen.csv).\n",
        "2High-Resolution: Direct figshare CSV at https://figshare.com/articles/dataset/2)A_High-resolution_Electric_Vehicle_Charging_Transaction_Dataset_with_Multidimensional_Features_in_China/28182251 (download link, but content not fetched fully via browser.\n",
        "3)\n",
        "Multi-Faceted: Figshare CSV at https://figshare.com/articles/dataset/A_dataset_for_multi-faceted_analysis_of_electric_vehicle_charging_transactions/22495141 (ChargingRecords.csv).\n",
        "4)ACN-Data: API-based, no direct CSV; samples via https://ev.caltech.edu/dataset.\n",
        "EV WATTS: Public DB at https://openenergyhub.ornl.gov/explore/dataset/ev-watts/ (CSVs available post-login).\n",
        "Others: No dire\n"
      ],
      "metadata": {
        "id": "1CGPnjPq6PyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample data from query (as text CSVs)\n",
        "samples = {\n",
        "    '1': 'timestamp,city,charger_id,duration_minutes,volume_kwh,electricity_price_usd,service_price_usd,weather_temp_c,weather_condition,poi_proximity_km,population_density_per_sqkm\\n2023-04-01 00:00,Shenzhen,charger_001,45,12.5,0.15,0.05,22.3,Cloudy,0.5,5000',\n",
        "    '2': 'transaction_id,start_time,end_time,duration_seconds,energy_kwh,price_usd,status,termination_reason,weather_temp_c,weather_condition\\ntrans_001,2023-01-01 08:00:00,2023-01-01 08:45:00,2700,15.2,2.28,completed,user_stop,18.5,Rainy',\n",
        "    '3': 'station_id,timestamp,occupancy,duration_hours,volume_kwh,price_usd,weather_temp_c,spatial_proximity_km\\nstation_123,2022-09-01 10:00,0.75,1.2,8.5,1.28,25.0,0.3',\n",
        "    '4': 'ChargingsessionID,UserID,ChargerID,ChargerCompany,Location,ChargerType,ChargerCapacity,ChargerACDC,StartDay,StartTime,EndDay,EndTime,StartDatetime,EndDatetime,Duration,Demand\\nsess_001,user_001,charger_001,CompanyA,Residential,Level2,7kW,AC,2022-01-01,08:00,2022-01-01,09:00,2022-01-01T08:00:00,2022-01-01T09:00:00,60,7.0',\n",
        "    '5': '_id,connectionTime,disconnectTime,doneChargingTime,kWhDelivered,sessionID,userID,userInputs_WhPerMile,userInputs_kWhRequested,userInputs_requestedDeparture\\nsess_123,2023-01-01T10:00:00Z,2023-01-01T12:00:00Z,2023-01-01T11:59:00Z,10.5,sess123,user556,250,10,2023-01-01T12:00:00Z',\n",
        "    '6': 'session_id,charger_id,start_time,end_time,energy_kwh,user_type,additional_info_driving_pattern_km\\nsess_001,charger_001,2024-01-01 08:00,2024-01-01 09:00,7.5,residential,45',\n",
        "    '7': 'user_id,station_id,charging_time_minutes,preference_rating,user_feedback\\nuser_001,station_001,30,4.5,\"Convenient location\"'\n",
        "}\n",
        "\n",
        "# Parse to DFs\n",
        "dfs = {k: pd.read_csv(StringIO(v)) for k, v in samples.items()}\n",
        "print('Samples parsed')"
      ],
      "metadata": {
        "id": "nbutIbCi8gZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Target features\n",
        "features = ['station_name', 'distance_km', 'power_kw', 'rating', 'cost_usd']\n",
        "\n",
        "# Mapping dict for fields to features\n",
        "field_map = {\n",
        "    'charger_id': 'station_name', 'station_id': 'station_name', 'ChargerID': 'station_name',\n",
        "    'poi_proximity_km': 'distance_km', 'spatial_proximity_km': 'distance_km', 'additional_info_driving_pattern_km': 'distance_km',\n",
        "    'ChargerCapacity': 'power_kw', 'volume_kwh': 'power_kw', 'energy_kwh': 'power_kw',\n",
        "    'preference_rating': 'rating',\n",
        "    'electricity_price_usd': 'cost_usd', 'price_usd': 'cost_usd', 'service_price_usd': 'cost_usd'  # Combine prices\n",
        "}\n",
        "\n",
        "# RAG-like: Vectorize columns, query for matches\n",
        "vectorizer = TfidfVectorizer()\n",
        "all_cols = [col for df in dfs.values() for col in df.columns]\n",
        "X = vectorizer.fit_transform(all_cols)\n",
        "\n",
        "def rag_extract(query):\n",
        "    q_vec = vectorizer.transform([query])\n",
        "    sims = cosine_similarity(q_vec, X)[0]\n",
        "    top_idx = np.argmax(sims)\n",
        "    return all_cols[top_idx]\n",
        "\n",
        "# Extract and build combined data\n",
        "combined_data = []\n",
        "for idx, df in dfs.items():\n",
        "    row = df.iloc[0]\n",
        "    new_row = {f: np.nan for f in features}\n",
        "    new_row['source_dataset'] = idx\n",
        "    for col in df.columns:\n",
        "        mapped = field_map.get(col, None)\n",
        "        if mapped:\n",
        "            new_row[mapped] = row[col]\n",
        "    # Special: Combine costs if multiple\n",
        "    if 'electricity_price_usd' in row and 'service_price_usd' in row:\n",
        "        new_row['cost_usd'] = row['electricity_price_usd'] + row['service_price_usd']\n",
        "    # Infer power from kWh if capacity missing (simplistic)\n",
        "    if pd.isna(new_row['power_kw']) and 'energy_kwh' in row:\n",
        "        new_row['power_kw'] = row['energy_kwh']  # Placeholder\n",
        "    combined_data.append(new_row)\n",
        "\n",
        "combined_df = pd.DataFrame(combined_data)\n",
        "combined_df.to_csv('combined_ev_dataset.csv', index=False)\n",
        "print(combined_df)"
      ],
      "metadata": {
        "id": "Y23WJYHl8qDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *prepare* *dataset* *for* *fine* *tuning* with *optimization* *and* *tokenization*"
      ],
      "metadata": {
        "id": "3N2csYgBZ_gj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PuLP"
      ],
      "metadata": {
        "id": "BhW7cdeSJQO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load/fix combined dataset (from previous, with weather augmentation)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import os # Import os for fallback\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "from datasets import Dataset\n",
        "import pulp  # NEW: For LP optimization\n",
        "\n",
        "combined_df = pd.read_csv('combined_ev_dataset.csv')  # Assume exists; else recreate as before\n",
        "\n",
        "# API Keys (replace) - Ensure these are handled securely, e.g., via Colab Secrets\n",
        "OCM_API_KEY = \"8ea88008-6ca4-40f1-89af-d66d1cc66cb5\"\n",
        "GOOGLE_API_KEY = \"AIzaSyCRDsy_Bt5UZO2Slho8bxLAmB6du5sdmiU\"\n",
        "\n",
        "# -------- UsageCost Parser --------\n",
        "def parse_usage_cost(cost_str: str):\n",
        "    if not cost_str or str(cost_str).strip().lower() == \"free\":\n",
        "        return {\"status\": \"Free\", \"rates\": [], \"avg_cost\": 0.0}\n",
        "\n",
        "    cost_str = str(cost_str).strip()\n",
        "    rates = []\n",
        "\n",
        "    # Per-minute\n",
        "    for match in re.findall(r\"(\\d+-?\\d*\\s*kW)?\\s*\\$([\\d.]+)\\/min\", cost_str):\n",
        "        range_kw, price = match\n",
        "        rates.append(float(price))\n",
        "\n",
        "    # Per-kWh\n",
        "    for match in re.findall(r\"(\\d+-?\\d*\\s*kW)?\\s*\\$([\\d.]+)\\/kWh\", cost_str):\n",
        "        range_kw, price = match\n",
        "        rates.append(float(price))\n",
        "\n",
        "    # Flat/session fee\n",
        "    for match in re.findall(r\"\\$([\\d.]+)\\s*(session|flat|fee)\", cost_str, flags=re.I):\n",
        "        price, _ = match\n",
        "        rates.append(float(price))\n",
        "\n",
        "    if rates:\n",
        "        avg_cost = np.mean(rates)  # normalize into one representative value\n",
        "        return {\"status\": \"Paid\", \"rates\": rates, \"avg_cost\": avg_cost}\n",
        "\n",
        "    return {\"status\": \"Unknown\", \"rates\": [], \"avg_cost\": 0.2}\n",
        "\n",
        "# Weather API: Use Open-Meteo (free, no key, from tool snippets)\n",
        "def fetch_weather(lat, lon):\n",
        "    url = f\"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current=weather_code,temperature\"\n",
        "    try:\n",
        "        resp = requests.get(url).json()\n",
        "        code = resp['current']['weather_code']\n",
        "        temp = resp['current']['temperature']\n",
        "        weather_score = 1.0 if code < 3 else 0.5\n",
        "        return weather_score, temp\n",
        "    except:\n",
        "        return 0.8, 20.0  # Mock fallback\n",
        "\n",
        "# -------- Mock fallback stations --------\n",
        "def mock_stations():\n",
        "    return pd.DataFrame([\n",
        "        {'station_name': 'Kamloops, BC Supercharger', 'distance_km': 1.8, 'power_kw': 150, 'rating': 4.7, 'cost_usd': 0.44, 'status': 'Operational', 'weather_score': 1.0},\n",
        "        {'station_name': 'Kamloops Canadian Tire - Electrify Canada', 'distance_km': 1.6, 'power_kw': 150, 'rating': 2.6, 'cost_usd': 0.50, 'status': 'Operational', 'weather_score': 0.5},\n",
        "        {'station_name': 'Fairfield Inn & Suites Kamloops', 'distance_km': 1.2, 'power_kw': 6.6, 'rating': 4.5, 'cost_usd': 0.0, 'status': 'Operational', 'weather_score': 1.0},\n",
        "    ])\n",
        "\n",
        "# -------- Fetch stations with weather + wait/charge time --------\n",
        "def fetch_stations(user_lat, user_lon, radius_km=10):\n",
        "    try:\n",
        "        url = f\"https://api.openchargemap.io/v3/poi/?output=json&latitude={user_lat}&longitude={user_lon}&distance={radius_km}&distanceunit=km&maxresults=50&key={OCM_API_KEY}\"\n",
        "        data = requests.get(url).json()\n",
        "        stations = []\n",
        "        for poi in data:\n",
        "            addr = poi['AddressInfo']\n",
        "            conn = poi['Connections'][0] if poi.get('Connections') and len(poi['Connections']) > 0 else {}\n",
        "            lat, lon = addr['Latitude'], addr['Longitude']\n",
        "            weather_score, _ = fetch_weather(lat, lon)\n",
        "            usage_info = parse_usage_cost(poi.get('UsageCost', ''))\n",
        "            cost_usd = usage_info[\"avg_cost\"]\n",
        "\n",
        "            station = {\n",
        "                'station_name': addr['Title'],\n",
        "                'distance_km': addr.get('Distance', np.random.uniform(0.5, 5)),\n",
        "                'power_kw': conn.get('PowerKW', 50),\n",
        "                'rating': np.nan,\n",
        "                'cost_usd': cost_usd,\n",
        "                'status': poi['StatusType'].get('Title', 'Operational') if 'StatusType' in poi else 'Operational',\n",
        "                'weather_score': weather_score,\n",
        "                'wait_time_est': 10 / np.random.uniform(2,5),\n",
        "            }\n",
        "            stations.append(station)\n",
        "        df_live = pd.DataFrame(stations)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching live data: {e}\")\n",
        "        df_live = mock_stations()\n",
        "\n",
        "    # --- Add wait/charge/total time ---\n",
        "    for i, row in df_live.iterrows():\n",
        "        df_live.at[i, 'wait_time_est'] = 10 / row['rating'] if row['rating'] > 0 else 5\n",
        "    df_live['battery_needed'] = 50\n",
        "    df_live['efficiency'] = 0.9\n",
        "    df_live['charge_time_min'] = (df_live['battery_needed'] / df_live['power_kw']) * 60 / df_live['efficiency']\n",
        "    df_live['total_time_min'] = df_live['charge_time_min'] + df_live['wait_time_est']\n",
        "\n",
        "    return df_live\n",
        "\n",
        "# -------- Augment ratings (Google Places) --------\n",
        "def augment_ratings(df_live, user_lat, user_lon):\n",
        "    if not GOOGLE_API_KEY or GOOGLE_API_KEY == 'AIzaSyCRDsy_Bt5UZO2Slho8bxLAmB6du5sdmiU':\n",
        "        print(\"Skipping Google Places rating augmentation: GOOGLE_API_KEY not set or is default.\")\n",
        "        return df_live\n",
        "\n",
        "    for i, row in df_live.iterrows():\n",
        "        if pd.isna(row['station_name']) or not isinstance(row['station_name'], str) or not row['station_name'].strip():\n",
        "            continue\n",
        "        query = f\"{row['station_name']} EV charger\"\n",
        "        url = f\"https://maps.googleapis.com/maps/api/place/textsearch/json?query={requests.utils.quote(query)}&location={user_lat},{user_lon}&radius=10000&key={GOOGLE_API_KEY}\"\n",
        "        try:\n",
        "            resp = requests.get(url).json()\n",
        "            if resp['status'] == 'OK' and resp['results']:\n",
        "                place_id = resp['results'][0]['place_id']\n",
        "                details_url = f\"https://maps.googleapis.com/maps/api/place/details/json?place_id={place_id}&fields=rating&key={GOOGLE_API_KEY}\"\n",
        "                details_resp = requests.get(details_url).json()\n",
        "                if details_resp['status'] == 'OK' and 'result' in details_resp:\n",
        "                    details = details_resp['result']\n",
        "                    if 'rating' in details:\n",
        "                        df_live.at[i, 'rating'] = details['rating']\n",
        "        except Exception as e:\n",
        "            print(f\"Error during Google Places API call for '{row['station_name']}': {e}\")\n",
        "            pass\n",
        "    return df_live\n",
        "user_lat, user_lon = 50.640054, -120.378926\n",
        "df_live = fetch_stations(user_lat, user_lon)\n",
        "df_live = augment_ratings(df_live, user_lat, user_lon)\n",
        "full_df = pd.concat([combined_df, df_live], ignore_index=True)\n",
        "\n",
        "# Fill NaNs column-wise to fix error - improved handling\n",
        "fill_values = {\n",
        "    'power_kw': 50.0, # Use float for consistency\n",
        "    'rating': 4.0,\n",
        "    'cost_usd': 0.2,\n",
        "    'distance_km': 2.0,\n",
        "    'weather_score': 0.8,\n",
        "    'status': 'Operational',\n",
        "    'station_name': 'Unknown Station' # Fill station_name NaNs\n",
        "}\n",
        "\n",
        "for col, val in fill_values.items():\n",
        "    if col in full_df.columns:\n",
        "        # Ensure correct data type before filling\n",
        "        if isinstance(val, (int, float)):\n",
        "            full_df[col] = pd.to_numeric(full_df[col], errors='coerce').fillna(val)\n",
        "        else:\n",
        "            full_df[col] = full_df[col].fillna(val)\n",
        "    else:\n",
        "        # Add column if it doesn't exist and fill with default\n",
        "        full_df[col] = val\n",
        "\n",
        "# Ensure 'power_kw' is numeric, coercing errors\n",
        "full_df['power_kw'] = pd.to_numeric(full_df['power_kw'], errors='coerce').fillna(fill_values['power_kw'])\n",
        "# Ensure other numeric columns are correct types\n",
        "full_df['distance_km'] = pd.to_numeric(full_df['distance_km'], errors='coerce').fillna(fill_values['distance_km'])\n",
        "full_df['rating'] = pd.to_numeric(full_df['rating'], errors='coerce').fillna(fill_values['rating'])\n",
        "full_df['cost_usd'] = pd.to_numeric(full_df['cost_usd'], errors='coerce').fillna(fill_values['cost_usd'])\n",
        "full_df['weather_score'] = pd.to_numeric(full_df['weather_score'], errors='coerce').fillna(fill_values['weather_score'])\n",
        "\n",
        "\n",
        "# Format for fine-tuning (add weather in prompts)\n",
        "data = []\n",
        "for _, row in full_df.iterrows():\n",
        "    # Ensure values are strings for formatting the prompt\n",
        "    power_str = str(row['power_kw']) if pd.notna(row['power_kw']) else '50'\n",
        "    cost_str = str(row['cost_usd']) if pd.notna(row['cost_usd']) else '0.2'\n",
        "    # Simple weather description based on score\n",
        "    weather_desc = \"good weather\" if row['weather_score'] > 0.7 else \"average weather\"\n",
        "\n",
        "    query = f\"Find fast charging near me with high rating, power >{power_str}kW, cost <{cost_str}, {weather_desc}\"\n",
        "    # Example completion - this part might need adjustment based on desired output format\n",
        "    # Assuming a simple dictionary output as before\n",
        "    completion = f\"{{'distance': 0.25, 'power': 0.25, 'rating': 0.2, 'price': 0.2, 'weather': 0.1}}\"  # Added weather weight - example weights\n",
        "\n",
        "    data.append({\"prompt\": f\"Extract preferences from: {query}. Output as dict:\", \"completion\": completion})\n",
        "\n",
        "train_data, val_data = train_test_split(data, test_size=0.1, random_state=42) # Added random_state for reproducibility\n",
        "train_ds = Dataset.from_list(train_data)\n",
        "val_ds = Dataset.from_list(val_data)\n",
        "\n",
        "\n",
        "    # -------- Optimization with pulp --------\n",
        "def optimize_station_selection(df):\n",
        "    prob = pulp.LpProblem(\"EV_Station_Selection\", pulp.LpMinimize)\n",
        "    x = pulp.LpVariable.dicts(\"x\", df.index, lowBound=0, upBound=1, cat=\"Binary\")\n",
        "\n",
        "    prob += pulp.lpSum([\n",
        "        (0.25 * df.loc[i, 'distance_km'] +\n",
        "         0.25 * df.loc[i, 'cost_usd'] +\n",
        "         0.25 * df.loc[i, 'total_time_min'] +\n",
        "         0.25 * (5 - df.loc[i, 'rating'])) * x[i]\n",
        "        for i in df.index\n",
        "    ])\n",
        "\n",
        "    prob += pulp.lpSum([x[i] for i in df.index]) == 1\n",
        "    prob.solve(pulp.PULP_CBC_CMD(msg=0))\n",
        "\n",
        "    chosen_idx = [i for i in df.index if pulp.value(x[i]) == 1][0]\n",
        "    return df.loc[chosen_idx]\n",
        "\n",
        "\n",
        "# Example optimization run\n",
        "best_station = optimize_station_selection(df_live)\n",
        "print(\"Best station selected:\\n\", best_station[['station_name','distance_km','power_kw','rating','cost_usd','total_time_min']])\n",
        "\n",
        "\n",
        "# Assuming 'tokenizer' is defined in a previous cell\n",
        "# If not, you would need to define or import it here\n",
        "# Example:\n",
        "# from transformers import AutoTokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"some_model_name\") # Replace with your actual tokenizer model\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # Ensure inputs and targets are strings\n",
        "    prompts = [str(p) for p in examples['prompt']]\n",
        "    completions = [str(c) for c in examples['completion']]\n",
        "    return tokenizer(prompts, text_target=completions, truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "\n",
        "# Check if tokenizer is defined before mapping\n",
        "if 'tokenizer' in locals() or 'tokenizer' in globals():\n",
        "    train_ds = train_ds.map(tokenize_function, batched=True)\n",
        "    val_ds = val_ds.map(tokenize_function, batched=True)\n",
        "    print(\"Datasets tokenized successfully.\")\n",
        "else:\n",
        "    print(\"Tokenizer is not defined. Please ensure the tokenizer is loaded in a previous cell.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Y4DcBztQ54Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Hyper parameter* *optimization* with text *optuna*"
      ],
      "metadata": {
        "id": "STSPempXanPK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####LoRA (Low-Rank Adaptation)\n",
        "\n",
        "1.Instead of updating all billions of parameters in an LLM, LoRA adds small trainable matrices (adapters) to certain layers (q_proj, v_proj, k_proj in attention).\n",
        "\n"
      ],
      "metadata": {
        "id": "fZWRmrQUb6b_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2..This is LoRA-based fine-tuning of a large language model (LLM) using PEFT (Parameter-Efficient Fine-Tuning) and Optuna (hyperparameter optimization).\n",
        "\n",
        "3.The goal is to train only a small set of parameters (LoRA adapters) instead of the full model, making training faster, cheaper, and memory-efficient."
      ],
      "metadata": {
        "id": "drWD3BqobrV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SOLUTION 1: Aggressive Memory Optimization\n",
        "import torch\n",
        "import gc\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import optuna\n",
        "\n",
        "# Clear GPU memory first\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Enable memory optimization\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Aggressive memory clearing\"\"\"\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "def objective(trial):\n",
        "    clear_memory()  # Clear memory at start of each trial\n",
        "\n",
        "    # Even more conservative hyperparameters\n",
        "    lr = trial.suggest_loguniform('lr', 5e-6, 1e-4)\n",
        "    r = trial.suggest_int('r', 4, 8)  # Very low rank\n",
        "    epochs = trial.suggest_int('epochs', 1, 2)  # Minimal epochs\n",
        "\n",
        "    # Ultra-conservative LoRA config\n",
        "    peft_config = LoraConfig(\n",
        "        r=r,\n",
        "        lora_alpha=16,  # Reduced from 32\n",
        "        lora_dropout=0.1,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],  # Only 2 modules instead of 3\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Prepare model with maximum memory savings\n",
        "        model_prepared = prepare_model_for_kbit_training(\n",
        "            model,\n",
        "            use_gradient_checkpointing=True,\n",
        "            gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
        "        )\n",
        "\n",
        "        model_peft = get_peft_model(model_prepared, peft_config)\n",
        "\n",
        "        # Ultra-conservative training arguments\n",
        "        args = TrainingArguments(\n",
        "            output_dir=\"./trials\",\n",
        "            num_train_epochs=epochs,\n",
        "            per_device_train_batch_size=1,\n",
        "            gradient_accumulation_steps=8,  # Increased to maintain effective batch size\n",
        "            learning_rate=lr,\n",
        "            weight_decay=0.01,\n",
        "            warmup_ratio=0.03,\n",
        "            optim=\"adamw_8bit\",  # Use 8-bit optimizer\n",
        "            evaluation_strategy=\"epoch\",  # Skip evaluation during trials\n",
        "            save_strategy=\"no\",\n",
        "            fp16=True,\n",
        "            dataloader_pin_memory=False,  # Disable pin memory\n",
        "            dataloader_num_workers=0,  # No parallel data loading\n",
        "            remove_unused_columns=False,\n",
        "            report_to=\"none\",\n",
        "            max_grad_norm=1.0,\n",
        "            ddp_find_unused_parameters=False\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model_peft,\n",
        "            args=args,\n",
        "            train_dataset=train_ds\n",
        "            # No eval_dataset to save memory\n",
        "        )\n",
        "\n",
        "        # Train with memory monitoring\n",
        "        trainer.train()\n",
        "\n",
        "        # save the pretarined model\n",
        "        model.save_pretrained(\"./trials\")\n",
        "        # save the tokenize model\n",
        "        tokenizer.save_pretrained(\"./trials\")\n",
        "        # Simple loss calculation instead of full evaluation\n",
        "        train_loss = trainer.state.log_history[-1].get('train_loss', float('inf'))\n",
        "\n",
        "        # Clean up immediately\n",
        "        del model_peft, trainer, args\n",
        "        clear_memory()\n",
        "\n",
        "        return train_loss\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        if \"out of memory\" in str(e).lower():\n",
        "            print(f\"OOM in trial: r={r}, lr={lr:.2e}, epochs={epochs}\")\n",
        "            clear_memory()\n",
        "            return float('inf')\n",
        "        else:\n",
        "            raise e\n",
        "    except Exception as e:\n",
        "        print(f\"Other error: {e}\")\n",
        "        clear_memory()\n",
        "        return float('inf')\n",
        "\n",
        "# Run optimization with minimal trials\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=2)  # Only 2 trials\n",
        "\n",
        "print(\"Best params:\", study.best_params)"
      ],
      "metadata": {
        "id": "sUS6eJ-jQ4Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *implementing* *sentence* *transformer*  *for* *fine* tuning *the llms* using pulp optimization"
      ],
      "metadata": {
        "id": "H9e4uLOreJF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llm"
      ],
      "metadata": {
        "id": "Fcf9lHB5R6wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create LLM Pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# Load the base model first\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"unsloth/llama-3-8b-Instruct\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load the PEFT model (LoRA adapters)\n",
        "try:\n",
        "    model = PeftModel.from_pretrained(base_model, \"./trials\")\n",
        "    logger.info(\"PEFT model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error loading PEFT model: {e}\")\n",
        "    # Fallback to base model if PEFT loading fails\n",
        "    model = base_model\n",
        "    logger.warning(\"Falling back to base model.\")\n",
        "\n",
        "\n",
        "# Create the pipeline using the loaded model\n",
        "llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_length=200, pad_token_id=tokenizer.pad_token_id)\n",
        "logger.info(\"LLM pipeline created successfully.\")"
      ],
      "metadata": {
        "id": "LHXnTvdJWHEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, gc\n",
        "import numpy as np\n",
        "import pulp\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ---- Free GPU memory before loading ----\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Run SentenceTransformer on CPU to avoid CUDA OOM\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
        "\n",
        "\n",
        "# ---------------- Process Query ----------------\n",
        "def process_query(query):\n",
        "    prompt = f\"Extract preferences from: {query}. Output as dict: {{'distance': weight, 'power': weight, 'rating': weight, 'price': weight, 'time': weight, 'weather': weight}}\"\n",
        "    response = llm(prompt)[0]['generated_text']\n",
        "    try:\n",
        "        prefs = eval(response.split(\"dict:\")[-1].strip())\n",
        "    except:\n",
        "        prefs = {'distance': 0.166, 'power': 0.166, 'rating': 0.166, 'price': 0.166, 'time': 0.166, 'weather': 0.166}\n",
        "    weights = np.array(list(prefs.values()))\n",
        "    weights /= weights.sum()\n",
        "\n",
        "    battery_prompt = f\"Extract battery needed kWh from: {query}. Output: number or 50\"\n",
        "    battery_resp = llm(battery_prompt)[0]['generated_text']\n",
        "    try:\n",
        "        battery_needed = float(battery_resp.strip())\n",
        "    except:\n",
        "        battery_needed = 50\n",
        "    return weights, prefs, battery_needed\n",
        "\n",
        "# ---------------- Compute Similarity ----------------\n",
        "def compute_similarity(weights, stations_df):\n",
        "    vecs = stations_df[['distance_km', 'power_kw', 'rating', 'cost_usd', 'total_time_min']].values.astype(float)\n",
        "    vecs[:, 0] = -vecs[:, 0]  # Invert distance (lower better)\n",
        "    vecs[:, 3] = -vecs[:, 3]  # Invert cost\n",
        "    vecs[:, 4] = -vecs[:, 4]  # Invert time\n",
        "    # Normalize columns\n",
        "    for j in range(vecs.shape[1]):\n",
        "        min_v, max_v = vecs[:, j].min(), vecs[:, j].max()\n",
        "        if max_v > min_v:\n",
        "            vecs[:, j] = (vecs[:, j] - min_v) / (max_v - min_v)\n",
        "    sim_scores = np.dot(vecs, weights) / (np.linalg.norm(vecs, axis=1) * np.linalg.norm(weights))\n",
        "    return sim_scores\n",
        "\n",
        "# ---------------- Optimization with PuLP ----------------\n",
        "def optimize_stations(stations_df, weights, battery_needed=50, constraints={'d_max': 5, 'p_max': 0.5, 'c_min': 50, 't_max': 30}, top_k=5, alpha=0.5):\n",
        "    stations_df['charge_time_min'] = (battery_needed / stations_df['power_kw']) * 60 / stations_df['efficiency']\n",
        "    stations_df['total_time_min'] = stations_df['charge_time_min'] + stations_df['wait_time_est']\n",
        "\n",
        "    prob = pulp.LpProblem(\"EV_Rec_Opt\", pulp.LpMaximize)\n",
        "    n = len(stations_df)\n",
        "    x = pulp.LpVariable.dicts(\"select\", range(n), cat='Binary')\n",
        "\n",
        "    vecs = stations_df[['distance_km', 'power_kw', 'rating', 'cost_usd', 'total_time_min']].values.astype(float)\n",
        "    scores = stations_df.apply(lambda row: sum(weights[j] * vecs[i, j] for j in range(len(weights))), axis=1)\n",
        "\n",
        "    prob += pulp.lpSum([scores[i] * x[i] for i in range(n)])\n",
        "    prob += pulp.lpSum([x[i] for i in range(n)]) == top_k\n",
        "\n",
        "    for i in range(n):\n",
        "        row = stations_df.iloc[i]\n",
        "        if row['distance_km'] > constraints['d_max']:\n",
        "            prob += x[i] == 0\n",
        "        if row['cost_usd'] > constraints['p_max']:\n",
        "            prob += x[i] == 0\n",
        "        if row['power_kw'] < constraints['c_min']:\n",
        "            prob += x[i] == 0\n",
        "        if row['total_time_min'] > constraints['t_max']:\n",
        "            prob += x[i] == 0\n",
        "        if row['status'] != 'Operational':\n",
        "            prob += x[i] == 0\n",
        "\n",
        "    prob.solve(pulp.PULP_CBC_CMD(msg=0))\n",
        "    selected_idx = [i for i in range(n) if pulp.value(x[i]) == 1]\n",
        "    ranked_df = stations_df.iloc[selected_idx].sort_values(by='total_time_min')\n",
        "    return ranked_df\n",
        "\n",
        "# ---------------- Generate Recommendations ----------------\n",
        "def generate_recommendations(ranked_df):\n",
        "    prompt = f\"Rank these EV stations with explanations, including est. charge time: {ranked_df.to_dict('records')}. Format as ### Recommended Charging Stations\\n#1. Name: details\"\n",
        "    response = llm(prompt)[0]['generated_text']\n",
        "    return response\n",
        "\n",
        "# ---------------- Update Weights ----------------\n",
        "def update_weights(weights, feedback, eta=0.1):\n",
        "    prob = pulp.LpProblem(\"Weight_Update\", pulp.LpMaximize)\n",
        "    delta = pulp.LpVariable.dicts(\"delta\", range(len(weights)), lowBound=-0.1, upBound=0.1)\n",
        "    prob += pulp.lpSum([feedback[j] * delta[j] for j in range(len(weights))])\n",
        "    prob += pulp.lpSum([(weights[j] + eta * delta[j]) for j in range(len(weights))]) == 1\n",
        "    for j in range(len(weights)):\n",
        "        prob += weights[j] + eta * delta[j] >= 0.05\n",
        "    prob.solve(pulp.PULP_CBC_CMD(msg=0))\n",
        "    new_weights = [weights[j] + eta * pulp.value(delta[j]) for j in range(len(weights))]\n",
        "    return np.array(new_weights)\n"
      ],
      "metadata": {
        "id": "IC6yZoDTTUgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Full* *pipeline* *and* *testing*"
      ],
      "metadata": {
        "id": "wtOxi7hzfEVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Intial Run\n",
        "query = \"fast charging high rated\"\n",
        "w, prefs,battery_needed = process_query(query)\n",
        "stations_df = full_df\n",
        "\n",
        "sims = compute_similarity(w, stations_df)\n",
        "ranked_df = optimize_stations(stations_df,w,battery_needed,top_k=5)\n",
        "rec_text = generate_recommendations(ranked_df)\n",
        "print(rec_text)\n",
        "\n",
        "# First  Feedback (e.g., user likes weather-aware rec)\n",
        "feedback = [0.05, 0.1, 0.1, -0.05, 0.1]  # Add for weather\n",
        "new_w = update_weights(w, feedback)\n",
        "print(\"Updated Weights:\", new_w)\n",
        "\n",
        "# Second Run with Updated Weights\n",
        "w = new_w  # Use updated weights for the next iteration\n",
        "sims = compute_similarity(w, stations_df)\n",
        "ranked_df = optimize_stations(stations_df, w, battery_needed,top_k=5)\n",
        "rec_text = generate_recommendations(ranked_df)\n",
        "print(\"Recommendations after First Feedback Update:\")\n",
        "print(rec_text)\n",
        "\n",
        "# Second Feedback Update (reinforce preference)\n",
        "feedback = [0.05, 0.15, 0.1, -0.1, 0.05, 0.15]  # Increase power/weather preference, reduce price\n",
        "new_w = update_weights(w, feedback)\n",
        "print(\"Updated Weights after Second Feedback:\", new_w)\n",
        "\n",
        "# Third Run with Updated Weights\n",
        "w = new_w  # Use updated weights\n",
        "sims = compute_similarity(w, stations_df)\n",
        "ranked_df = optimize_stations(stations_df, w, battery_needed)\n",
        "rec_text = generate_recommendations(ranked_df)\n",
        "print(\"Recommendations after Second Feedback Update:\")\n",
        "print(rec_text)\n"
      ],
      "metadata": {
        "id": "2frZ9Im7Zait"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}